{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning. Straighten your data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction & Motivation\n",
    "\n",
    "Binning is the common name for the set of approaches we can use for feature possible values range reducing. For example, we can split the continuous feature range into several parts and treat it like the categorical one. On the other side, we can merge some classes of the categorical variable. But why?\n",
    "\n",
    "First of all, the reasons depend on the type of feature we bin. For categorical ones, it's not good if we don't have enough observations in some classes. So our model won't be able to make confident predictions for this class (underfitting). Also binning is obviously effective mean against outliers in numerical features: every extremely huge or small characteristic value is hidden as another instance of the top or bottom bin.\n",
    "\n",
    "It's especially important that binning allows us to find and process non-monotonicity in numerical variables and hence we can make linear models work better without resorting to unstable and uninterpretable polynomial features.\n",
    "\n",
    "Another point of linear model improvement and the common advantage of the binning technique for both categorical and numerical features is missing values processing. Also, you can get a deeper understanding of your data and make feature selection using some special metrics for binning results evaluation.\n",
    "\n",
    "The most popular application of binning is a bank scorecard building. So in the tutorial, we will talk about the task - binary classification (are customer good or bad?) and the model - logistic regression. But I'm sure that these concepts are applicable for linear regression tasks too.\n",
    "\n",
    "So, how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Algorithms Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bank scoring theoretician Lyn C. Thomas said that binning technique \"is as much art as it is science\". That's true and obviously, there are many algorithms of binning.\n",
    "\n",
    "Trivial way is splitting into equal parts. We can do it cutting the range into intervals with equal width (same variable band into each bin) or with equal size (same number of observations).  But these methods don't count target distribution at all. Also, we don't have any criteria for the choice of optimal split-points.\n",
    "\n",
    "The more advanced path is to apply powerful statistical tests. The main idea is quite simple too.\n",
    "\n",
    "OK, we start with our previous step and for clarity let's split feature range into a number of the smallest intervals (one observation in each bin). As we mentioned in the introduction we are deciding a binary classification task, such as prediction of bank delinquencies. So we can write a target value 1 or 2 (for good and bad clients respectively) in every small bin on a training set. After adding the column of feature values we should arrange our table in feature ascending order. Imagine that we got something like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for every pair of adjacent bins, we can build a contingency table. It presents a frequency distribution of instances among two binary variables: the bin in pair and the target value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics is starting! We are looking for similar pairs of adjacent bins to merge them, isn't it? So our null hypothesis stating is that bins have the same one-two frequency distribution and observed difference arose by chance. Let's compute expected values of frequency in this case (don't scare they can be fractions!) and check for every pair how far it from being siblings. We can do it using Pearson's chi-squared distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have to note that it is not a real statistical test because there are some constraints for Pearson's chi-squared test (we need more observations in each cell). But since we don't compute the p-value it is not very crucial. Apart from that, it's better to initialize bins with much more instances in each one in real-life cases.\n",
    "\n",
    "OK, we have calculated statistics for all the pairs. There is a zero division problem for when adjacent bins have equal target distribution but we skip it and consider Pearson's distance is zero in these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./4.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge the most similar bins (with minimal chi-squared) and recalculate statistics and repeat and repeat again until we have stepped some predefined threshold for the greatest chi-square distance. One could use p-value to set this threshold but it is important to remember about mentioned constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./5.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to apply another statistical test or other similarity measures. However, the main idea of optimal statistical binning is splitting the characteristic possible answer set into intervals which have the most different good-bad proportion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The described approach is widespread but it is not only one. There are tree-based algorithms: conditional inference trees or traditional decision trees with information gain as splitting criteria.\n",
    "\n",
    "If we have strong prior knowledge that the bad rate (or target rate, bad clients percentage) will be monotone in some variable then the best way is using Maximum likelihood monotone coarse classifier (MLMCC). It provides a strict linear dependence between predictor and bad rate i.e. the maximum likelihood splitting in full concordance with its name.\n",
    "\n",
    "OK, I hope you feel that there is not any rocket science in binning. So feel free to be an artist. You can combine some of these methods to do your best for decision your own task. Links to the useful sources where you can get more detail are in the references at the end of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WoE & IV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight of evidence (WoE) and Information Value (IV) are the most commonly used metrics for evaluation binning results.\n",
    "WoE is calculated for each separate bin and shows how much does target distribution in the bin distinct from distribution on the whole train set:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?WoE_i = \ln\left((\frac{G_i}{\sum\limits_{i=1}^n{G_i}}) / (\frac{B_i}{\sum\limits_{i=1}^n{B_i}}
)\right) \" />",
    "$$\\displaystyle WoE_i = \\ln\\left((\\frac{G_i}{\\sum\\limits_{i=1}^n{G_i}}) / (\\frac{B_i}{\\sum\\limits_{i=1}^n{B_i}}\n",
    ")\\right)$$\n",
    "\n",
    "Here $\\sum{G_i}$ and $\\sum{B_i}$ are amounts of good and bad instances in the ith bin. Obviously, if we put random samples in the bin we would get nearly the same distribution in it and WoE would be zero. The WoE range is from -inf to +inf, so it presents how far is our bin from just a random sample.\n",
    "\n",
    "To evaluate the predictive power of whole binned predictor one should use IV:\n",
    "\n",
    "$$\\displaystyle IV = \\sum\\limits_{i=1}^n{\\left(WoE_i*(\\frac{G_i}{\\sum\\limits_{i=1}^n{G_i}}-\\frac{B_i}{\\sum\\limits_{i=1}^n{B_i}})\\right)}$$\n",
    "\n",
    "By convention, we have some simple rules for predictor IV values: \n",
    "- less than 0.02: not useful predictor;\n",
    "- 0.02 to 0.1: weak predictor;\n",
    "- 0.1 to 0.3: medium predictor;\n",
    "- 0.3 to 0.5: strong predictor;\n",
    "- more than 0.5: suspicious predictor (should be checked for data leakage or some other mistakes).\n",
    "\n",
    "So, we can evaluate, compare and select features using IV.\n",
    "\n",
    "Another noteworthy trick is WoE-encoding of binned categorical predictors. It allows avoiding dataset explosion with a lot of dummy variables and it is similar to the target encoding technique. We just need to write in every bin its WoE value and launch logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I faced with binning at my job (of course, it's a bank) I was surprised that there wasn't suitable open source packages for it in Python. So I've coded it myself.\n",
    "\n",
    "You can find the full code of my implementation of binning in file **pwlf_binner.py**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of my algorithm is to approximate ROC-curve. We will lose the least amount if information about our data if we do so. You just should pass in constructor required number of bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./example.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, binner uses differential evolution minimisation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's check how it can help us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model training, we will use data about bank customers and their delinquencies from [Yorko's Git](https://github.com/Yorko/mlcourse.ai).\n",
    "\n",
    "We will drop all NaN from dataset so that we will be able to compare logistic regression performance on binned and unbinned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prerequirements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pwlf_binner import GiniBinner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./credit_scoring_train.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['Delinquent90']\n",
    "X_train, X_val, y_train, y_val = train_test_split(data.drop(columns=['Delinquent90', 'client_id']), target,\n",
    "                                                  test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Install tensorflow to have access to PiecewiseLinFitTF.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:00<00:00, 21.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 29s, sys: 22 s, total: 3min 51s\n",
      "Wall time: 2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "binner = GiniBinner(fast=True)\n",
    "X_train_binned = binner.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00,  9.44it/s]\n"
     ]
    }
   ],
   "source": [
    "X_val_binned = binner.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini is 0.35691583329347765\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict_proba(X_val)[:,1]\n",
    "print('Gini is', roc_auc_score(y_val, y_pred)*2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini is 0.6525408136381639\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_binned, y_train)\n",
    "y_pred = lr.predict_proba(X_val_binned)[:,1]\n",
    "print('Gini is', roc_auc_score(y_val, y_pred)*2 - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got great gain in our model quality!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
